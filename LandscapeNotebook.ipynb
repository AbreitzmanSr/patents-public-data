{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add header/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "GPU_MEM_CONFIG = tf.ConfigProto(gpu_options={'allow_growth': True})\n",
    "seed_file = 'seeds/3d_gesture.seed.csv'\n",
    "# BigQuery must be enabled for this project\n",
    "bq_project = 'patent-landscape-165715'\n",
    "patent_dataset = 'patents-public-data:patents.publications_latest'\n",
    "num_anti_seed_patents = 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use models/5.9m directory to load/persist model information.\n",
      "INFO:tensorflow:Restoring parameters from models/5.9m/checkpoints/5.9m_abstracts.ckpt-1325000\n"
     ]
    }
   ],
   "source": [
    "from expansion import PatentLandscapeExpander\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "expander = PatentLandscapeExpander(\n",
    "    seed_file,\n",
    "    bq_project=bq_project,\n",
    "    patent_dataset=patent_dataset,\n",
    "    num_antiseed=num_anti_seed_patents)\n",
    "\n",
    "\n",
    "word2vec5_9m = Word2Vec('5.9m')\n",
    "w2v_runtime = word2vec5_9m.restore_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for all US CPC Counts\n",
      "Querying for Seed Set CPC Counts\n",
      "Querying to find total number of US patents\n",
      "Got 26282 relevant seed refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (26282, 1), to patents._l1_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L1 expansion: (133137, 3)\n",
      "Got 522720 relevant L1->L2 refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (522720, 1), to patents._l2_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L2 expansion: (495565, 3)\n",
      "Size of union of [Seed, L1, and L2]: 547057\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (547057, 1), to patents.antiseed_tmp\n",
      "Completed loading temp table.\n",
      "Loading training data text from (16167, 2) publication numbers\n",
      "Loading dataframe with cols Index(['publication_number'], dtype='object'), shape (16167, 1), to patents._tmp_training\n",
      "Completed loading temp table.\n",
      "Loading patent texts from provided publication numbers.\n",
      "Merging labels into training data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_num</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>family_id</th>\n",
       "      <th>priority_date</th>\n",
       "      <th>title_text</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>description_text</th>\n",
       "      <th>refs</th>\n",
       "      <th>cpcs</th>\n",
       "      <th>ExpansionLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004116467</td>\n",
       "      <td>US-2004116467-A1</td>\n",
       "      <td>32094123</td>\n",
       "      <td>20021011</td>\n",
       "      <td>Hexahydro-benzimidazolone compounds useful as ...</td>\n",
       "      <td>The present invention is directed to compounds...</td>\n",
       "      <td>We claim:  \\n       \\n         1 . A compound ...</td>\n",
       "      <td>[0001]    This application claims benefit of p...</td>\n",
       "      <td>US-3930005-A,US-3930005-A,US-4925854-A,US-4925...</td>\n",
       "      <td>C07D403/10,C07D235/26,C07D403/10,C07D235/26,C0...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7822015</td>\n",
       "      <td>US-7822015-B2</td>\n",
       "      <td>31885289</td>\n",
       "      <td>20040211</td>\n",
       "      <td>Method of operating systems comprising communi...</td>\n",
       "      <td>A method of operatively handling data systems ...</td>\n",
       "      <td>1. A method of operatively handling data syste...</td>\n",
       "      <td>BACKGROUND OF THE INVENTION \\n     1. Field of...</td>\n",
       "      <td>EP-0928548-A1,EP-0928548-A1,EP-0928548-A1,EP-0...</td>\n",
       "      <td>H04Q2213/13349,H04Q3/0062,H04M3/42,H04Q2213/13...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006228038</td>\n",
       "      <td>US-2006228038-A1</td>\n",
       "      <td>32771501</td>\n",
       "      <td>20030228</td>\n",
       "      <td>Method and system for enhancing portrait image...</td>\n",
       "      <td>A batch processing method for enhancing an app...</td>\n",
       "      <td>1 - 34 . (canceled)  \\n   \\n       \\n       35...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...</td>\n",
       "      <td>US-5430809-A,US-5430809-A,US-5430809-A,US-5430...</td>\n",
       "      <td>G06T11/00,G06T2200/24,G06T5/005,G06T2207/30201...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5839393</td>\n",
       "      <td>US-5839393-A</td>\n",
       "      <td>25237050</td>\n",
       "      <td>19970324</td>\n",
       "      <td>Animal restraint jacket</td>\n",
       "      <td>An animal restraint jacket includes an elongat...</td>\n",
       "      <td>We claim: \\n       \\n       1. An animal restr...</td>\n",
       "      <td>TECHNICAL FIELD \\n     The present invention r...</td>\n",
       "      <td>US-1595834-A,US-1595834-A,US-4489676-A,US-4489...</td>\n",
       "      <td>A01K13/006,A61D3/00,A01K13/006,A61D3/00,A01K13...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6568999</td>\n",
       "      <td>US-6568999-B2</td>\n",
       "      <td>23454654</td>\n",
       "      <td>19990805</td>\n",
       "      <td>Method and apparatus for cleaning a surface of...</td>\n",
       "      <td>A method and apparatus for cleaning a surface ...</td>\n",
       "      <td>What is claimed is:  \\n       \\n       1. A me...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION \\n     ...</td>\n",
       "      <td>US-5144711-A,US-5144711-A,US-5144711-A,US-5806...</td>\n",
       "      <td>B08B1/04,H01L21/67046,B08B1/00,B08B1/04,H01L21...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_num publication_number family_id  priority_date  \\\n",
       "0  2004116467   US-2004116467-A1  32094123       20021011   \n",
       "1     7822015      US-7822015-B2  31885289       20040211   \n",
       "2  2006228038   US-2006228038-A1  32771501       20030228   \n",
       "3     5839393       US-5839393-A  25237050       19970324   \n",
       "4     6568999      US-6568999-B2  23454654       19990805   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  Hexahydro-benzimidazolone compounds useful as ...   \n",
       "1  Method of operating systems comprising communi...   \n",
       "2  Method and system for enhancing portrait image...   \n",
       "3                            Animal restraint jacket   \n",
       "4  Method and apparatus for cleaning a surface of...   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  The present invention is directed to compounds...   \n",
       "1  A method of operatively handling data systems ...   \n",
       "2  A batch processing method for enhancing an app...   \n",
       "3  An animal restraint jacket includes an elongat...   \n",
       "4  A method and apparatus for cleaning a surface ...   \n",
       "\n",
       "                                         claims_text  \\\n",
       "0  We claim:  \\n       \\n         1 . A compound ...   \n",
       "1  1. A method of operatively handling data syste...   \n",
       "2  1 - 34 . (canceled)  \\n   \\n       \\n       35...   \n",
       "3  We claim: \\n       \\n       1. An animal restr...   \n",
       "4  What is claimed is:  \\n       \\n       1. A me...   \n",
       "\n",
       "                                    description_text  \\\n",
       "0  [0001]    This application claims benefit of p...   \n",
       "1  BACKGROUND OF THE INVENTION \\n     1. Field of...   \n",
       "2  CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...   \n",
       "3  TECHNICAL FIELD \\n     The present invention r...   \n",
       "4  CROSS-REFERENCE TO RELATED APPLICATION \\n     ...   \n",
       "\n",
       "                                                refs  \\\n",
       "0  US-3930005-A,US-3930005-A,US-4925854-A,US-4925...   \n",
       "1  EP-0928548-A1,EP-0928548-A1,EP-0928548-A1,EP-0...   \n",
       "2  US-5430809-A,US-5430809-A,US-5430809-A,US-5430...   \n",
       "3  US-1595834-A,US-1595834-A,US-4489676-A,US-4489...   \n",
       "4  US-5144711-A,US-5144711-A,US-5144711-A,US-5806...   \n",
       "\n",
       "                                                cpcs ExpansionLevel  \n",
       "0  C07D403/10,C07D235/26,C07D403/10,C07D235/26,C0...       AntiSeed  \n",
       "1  H04Q2213/13349,H04Q3/0062,H04M3/42,H04Q2213/13...       AntiSeed  \n",
       "2  G06T11/00,G06T2200/24,G06T5/005,G06T2207/30201...       AntiSeed  \n",
       "3  A01K13/006,A61D3/00,A01K13/006,A61D3/00,A01K13...       AntiSeed  \n",
       "4  B08B1/04,H01L21/67046,B08B1/00,B08B1/04,H01L21...       AntiSeed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_full_df, seed_patents_df, l1_patents_df, l2_patents_df, anti_seed_patents = \\\n",
    "    expander.derive_training_data_from_seeds(seed_file)\n",
    "training_data_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance': 0.34334943253643091, 'index': 16271, 'word': 'theater'},\n",
       " {'distance': 0.42952651805699049, 'index': 22194, 'word': 'exhibition'},\n",
       " {'distance': 0.44251179096683857, 'index': 17523, 'word': 'movies'},\n",
       " {'distance': 0.47240372507879447, 'index': 18624, 'word': 'studio'},\n",
       " {'distance': 0.48089787571297071, 'index': 31126, 'word': 'theatrical'},\n",
       " {'distance': 0.48534206041519257, 'index': 33764, 'word': 'theaters'},\n",
       " {'distance': 0.49094524468954592, 'index': 11805, 'word': 'projectors'},\n",
       " {'distance': 0.49423883364655918, 'index': 30381, 'word': 'cinematographic'},\n",
       " {'distance': 0.49438433435760043, 'index': 9310, 'word': 'movie'},\n",
       " {'distance': 0.49444645955748223, 'index': 27391, 'word': 'filmed'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_embedding = w2v_runtime.load_embedding('test')\n",
    "w2v_runtime.find_similar('cinema', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(110240, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(w2v_runtime.embedding_weights))\n",
    "w2v_runtime.embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_num</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>family_id</th>\n",
       "      <th>priority_date</th>\n",
       "      <th>title_text</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>description_text</th>\n",
       "      <th>refs</th>\n",
       "      <th>cpcs</th>\n",
       "      <th>ExpansionLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004116467</td>\n",
       "      <td>US-2004116467-A1</td>\n",
       "      <td>32094123</td>\n",
       "      <td>20021011</td>\n",
       "      <td>Hexahydro-benzimidazolone compounds useful as ...</td>\n",
       "      <td>The present invention is directed to compounds...</td>\n",
       "      <td>We claim:  \\n       \\n         1 . A compound ...</td>\n",
       "      <td>[0001]    This application claims benefit of p...</td>\n",
       "      <td>US-3930005-A,US-3930005-A,US-4925854-A,US-4925...</td>\n",
       "      <td>C07D403/10,C07D235/26,C07D403/10,C07D235/26,C0...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7822015</td>\n",
       "      <td>US-7822015-B2</td>\n",
       "      <td>31885289</td>\n",
       "      <td>20040211</td>\n",
       "      <td>Method of operating systems comprising communi...</td>\n",
       "      <td>A method of operatively handling data systems ...</td>\n",
       "      <td>1. A method of operatively handling data syste...</td>\n",
       "      <td>BACKGROUND OF THE INVENTION \\n     1. Field of...</td>\n",
       "      <td>EP-0928548-A1,EP-0928548-A1,EP-0928548-A1,EP-0...</td>\n",
       "      <td>H04Q2213/13349,H04Q3/0062,H04M3/42,H04Q2213/13...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006228038</td>\n",
       "      <td>US-2006228038-A1</td>\n",
       "      <td>32771501</td>\n",
       "      <td>20030228</td>\n",
       "      <td>Method and system for enhancing portrait image...</td>\n",
       "      <td>A batch processing method for enhancing an app...</td>\n",
       "      <td>1 - 34 . (canceled)  \\n   \\n       \\n       35...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...</td>\n",
       "      <td>US-5430809-A,US-5430809-A,US-5430809-A,US-5430...</td>\n",
       "      <td>G06T11/00,G06T2200/24,G06T5/005,G06T2207/30201...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5839393</td>\n",
       "      <td>US-5839393-A</td>\n",
       "      <td>25237050</td>\n",
       "      <td>19970324</td>\n",
       "      <td>Animal restraint jacket</td>\n",
       "      <td>An animal restraint jacket includes an elongat...</td>\n",
       "      <td>We claim: \\n       \\n       1. An animal restr...</td>\n",
       "      <td>TECHNICAL FIELD \\n     The present invention r...</td>\n",
       "      <td>US-1595834-A,US-1595834-A,US-4489676-A,US-4489...</td>\n",
       "      <td>A01K13/006,A61D3/00,A01K13/006,A61D3/00,A01K13...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6568999</td>\n",
       "      <td>US-6568999-B2</td>\n",
       "      <td>23454654</td>\n",
       "      <td>19990805</td>\n",
       "      <td>Method and apparatus for cleaning a surface of...</td>\n",
       "      <td>A method and apparatus for cleaning a surface ...</td>\n",
       "      <td>What is claimed is:  \\n       \\n       1. A me...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION \\n     ...</td>\n",
       "      <td>US-5144711-A,US-5144711-A,US-5144711-A,US-5806...</td>\n",
       "      <td>B08B1/04,H01L21/67046,B08B1/00,B08B1/04,H01L21...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_num publication_number family_id  priority_date  \\\n",
       "0  2004116467   US-2004116467-A1  32094123       20021011   \n",
       "1     7822015      US-7822015-B2  31885289       20040211   \n",
       "2  2006228038   US-2006228038-A1  32771501       20030228   \n",
       "3     5839393       US-5839393-A  25237050       19970324   \n",
       "4     6568999      US-6568999-B2  23454654       19990805   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  Hexahydro-benzimidazolone compounds useful as ...   \n",
       "1  Method of operating systems comprising communi...   \n",
       "2  Method and system for enhancing portrait image...   \n",
       "3                            Animal restraint jacket   \n",
       "4  Method and apparatus for cleaning a surface of...   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  The present invention is directed to compounds...   \n",
       "1  A method of operatively handling data systems ...   \n",
       "2  A batch processing method for enhancing an app...   \n",
       "3  An animal restraint jacket includes an elongat...   \n",
       "4  A method and apparatus for cleaning a surface ...   \n",
       "\n",
       "                                         claims_text  \\\n",
       "0  We claim:  \\n       \\n         1 . A compound ...   \n",
       "1  1. A method of operatively handling data syste...   \n",
       "2  1 - 34 . (canceled)  \\n   \\n       \\n       35...   \n",
       "3  We claim: \\n       \\n       1. An animal restr...   \n",
       "4  What is claimed is:  \\n       \\n       1. A me...   \n",
       "\n",
       "                                    description_text  \\\n",
       "0  [0001]    This application claims benefit of p...   \n",
       "1  BACKGROUND OF THE INVENTION \\n     1. Field of...   \n",
       "2  CROSS-REFERENCE TO RELATED APPLICATION  \\n    ...   \n",
       "3  TECHNICAL FIELD \\n     The present invention r...   \n",
       "4  CROSS-REFERENCE TO RELATED APPLICATION \\n     ...   \n",
       "\n",
       "                                                refs  \\\n",
       "0  US-3930005-A,US-3930005-A,US-4925854-A,US-4925...   \n",
       "1  EP-0928548-A1,EP-0928548-A1,EP-0928548-A1,EP-0...   \n",
       "2  US-5430809-A,US-5430809-A,US-5430809-A,US-5430...   \n",
       "3  US-1595834-A,US-1595834-A,US-4489676-A,US-4489...   \n",
       "4  US-5144711-A,US-5144711-A,US-5144711-A,US-5806...   \n",
       "\n",
       "                                                cpcs ExpansionLevel  \n",
       "0  C07D403/10,C07D235/26,C07D403/10,C07D235/26,C0...       AntiSeed  \n",
       "1  H04Q2213/13349,H04Q3/0062,H04M3/42,H04Q2213/13...       AntiSeed  \n",
       "2  G06T11/00,G06T2200/24,G06T5/005,G06T2207/30201...       AntiSeed  \n",
       "3  A01K13/006,A61D3/00,A01K13/006,A61D3/00,A01K13...       AntiSeed  \n",
       "4  B08B1/04,H01L21/67046,B08B1/00,B08B1/04,H01L21...       AntiSeed  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = training_data_full_df[\n",
    "    ['publication_number', 'title_text', 'abstract_text', 'claims_text', 'description_text', 'ExpansionLevel', 'refs', 'cpcs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication_number    1092\n",
      "title_text            1092\n",
      "abstract_text         1092\n",
      "claims_text           1092\n",
      "description_text      1092\n",
      "ExpansionLevel        1092\n",
      "refs                  1092\n",
      "cpcs                  1092\n",
      "dtype: int64\n",
      "publication_number    9629\n",
      "title_text            9629\n",
      "abstract_text         9629\n",
      "claims_text           9629\n",
      "description_text      9629\n",
      "ExpansionLevel        9629\n",
      "refs                  9629\n",
      "cpcs                  9629\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(training_df[training_df.ExpansionLevel == 'Seed'].count())\n",
    "print(training_df[training_df.ExpansionLevel == 'AntiSeed'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "import tokenizer\n",
    "tokenizer = tokenizer.TextTokenizer()\n",
    "kTokenizer = text.Tokenizer(\n",
    "    num_words=50000,\n",
    "    split=\",\",\n",
    "    # filter should be same as default, minus the '-'\n",
    "    filters='!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=False)\n",
    "\n",
    "class TrainingData:\n",
    "    training_df = None\n",
    "    series_text_to_embed = None\n",
    "    prepped_embedding_train = None\n",
    "    prepped_refs = None\n",
    "    prepped_labels = None\n",
    "    w2v_runtime = None\n",
    "    ref_to_id = None\n",
    "    id_to_ref = None\n",
    "\n",
    "def label_text_to_id(label_name):\n",
    "    if label_name == 'antiseed':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def label_id_to_text(label_idx):\n",
    "    if label_idx == 1:\n",
    "        return 'antiseed'\n",
    "    else:\n",
    "        return 'seed'\n",
    "\n",
    "def tensor_label_to_text(tensor_text):\n",
    "    # if element 0 is '1', it means that this is the\n",
    "    # zeroth label index\n",
    "    if tensor_text[0] == 1.0:\n",
    "        return label_id_to_text(0)\n",
    "    return label_id_to_text(1)\n",
    "\n",
    "def prediction_to_label(predict_tensor):\n",
    "    return label_id_to_text(prediction_to_idx(predict_tensor))\n",
    "\n",
    "def prediction_to_idx(predict_tensor):\n",
    "    # if element 0 is greater than element 1, it's the label\n",
    "    if predict_tensor[0] > predict_tensor[1]:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def to_text(integerized):\n",
    "    words = []\n",
    "    for word_int in integerized:\n",
    "        words.append(w2v_runtime.index_to_word[word_int])\n",
    "    return ' '.join(words)\n",
    "\n",
    "def show_details(training_data_series, idx):\n",
    "    print('\\nOriginal: {}\\nTokenized: {}\\nIntegerized: {}\\nLabelIntegerized: {}'.format(\n",
    "        training_data_series[idx],\n",
    "        to_text(prepped_train[idx]),\n",
    "        prepped_train[idx],\n",
    "        prepped_labels[idx]))\n",
    "\n",
    "def prep_series_for_training(w2v_runtime, raw_series_text, labels_series):\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize_series(raw_series_text)\n",
    "    word_to_index_dict = w2v_runtime.word_to_index\n",
    "    tokenized_indexed_titles = []\n",
    "    labels_indexed = []\n",
    "\n",
    "    for idx in range(0, len(tokenized_text)):\n",
    "        title = tokenized_text[idx]\n",
    "        label = labels_series[idx]\n",
    "        title_word_indexes = []\n",
    "        for word in title:\n",
    "            if word in word_to_index_dict:\n",
    "                word_idx = word_to_index_dict[word]\n",
    "            else:\n",
    "                word_idx = word_to_index_dict['UNK']\n",
    "            # this skips 'the' so it can be used for dynamic rnn\n",
    "            if word_idx > 0:\n",
    "                title_word_indexes.append(word_idx)\n",
    "\n",
    "        tokenized_indexed_titles.append(title_word_indexes)\n",
    "        tokenized_label = tokenizer.tokenize(label)[0]\n",
    "        label_idx = label_text_to_id(tokenized_label)\n",
    "        labels_indexed.append(label_idx)\n",
    "\n",
    "    return tokenized_indexed_titles, labels_indexed\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenize_refs(refs_series, vocab_size=1000):\n",
    "    refs_counter = Counter()\n",
    "    for idx in range(0, len(refs_series)):\n",
    "        refs = refs_series[idx]\n",
    "        for ref in refs.split(','):\n",
    "            if ref.strip() is not '':\n",
    "                refs_counter[ref.strip()] += 1\n",
    "\n",
    "    if len(refs_counter) >= vocab_size:\n",
    "        print('reducing ref count from {} to top {} refs'.format(len(refs_counter), vocab_size))\n",
    "        refs_counter = OrderedDict(refs_counter.most_common(vocab_size - 1))\n",
    "\n",
    "    refs_counter['UNK'] = 1\n",
    "\n",
    "    sorted_ref_vocab = sorted(refs_counter, key=refs_counter.get, reverse=True)\n",
    "    id_to_ref = {ii: ref for ii, ref in enumerate(sorted_ref_vocab)}\n",
    "    ref_to_id = {ref: ii for ii, ref in id_to_ref.items()}\n",
    "\n",
    "    tokenized_refs = []\n",
    "    for idx in range(0, len(refs_series)):\n",
    "        refs = refs_series[idx]\n",
    "        refs_for_patent = []\n",
    "        for ref in refs.split(','):\n",
    "            if ref.strip() is not '' and ref in ref_to_id:\n",
    "                refs_for_patent.append(ref_to_id[ref])\n",
    "\n",
    "        tokenized_refs.append(refs_for_patent)\n",
    "\n",
    "    return tokenized_refs, id_to_ref, ref_to_id\n",
    "    # TODO: reconstruct series with ids instead of text of refs\n",
    "\n",
    "    #for idx in range(0, len(refs_series)):\n",
    "    #    for ref in refs.split(','):\n",
    "\n",
    "def ref_ids_to_text(training_data, ref_onehot):\n",
    "    refs = []\n",
    "    for idx in range(1, len(ref_onehot)+1):\n",
    "        if ref_onehot[idx-1] == 1.0:\n",
    "            refs.append(kTokenizer.index_word[idx])\n",
    "    #for ref_id in ref_ids:\n",
    "    #    if ref_id in training_data.id_to_ref:\n",
    "    #        refs.append(training_data.id_to_ref[ref_id])\n",
    "    return refs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TrainingData()\n",
    "td.kTokenizer = kTokenizer\n",
    "td.w2v_runtime = w2v_runtime\n",
    "td.training_df = training_df\n",
    "td.series_text_to_embed = training_df.abstract_text\n",
    "\n",
    "td.prepped_embedding_train, td.prepped_labels = \\\n",
    "    prep_series_for_training(td.w2v_runtime, td.series_text_to_embed, training_df.ExpansionLevel)\n",
    "\n",
    "kTokenizer.fit_on_texts(td.training_df.refs)\n",
    "kTokenizer.index_word = {idx: ref for ref, idx in kTokenizer.word_index.items()}\n",
    "td.refs_one_hot = kTokenizer.texts_to_matrix(td.training_df.refs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179207\n",
      "(10721, 50000)\n"
     ]
    }
   ],
   "source": [
    "#print(len(td.kTokenizer.word_index))\n",
    "#print(td.refs_one_hot.shape)\n",
    "#td.kTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: (10721, 8), num tokenized docs: 10721, num labels: 10721\n"
     ]
    }
   ],
   "source": [
    "#training_data_series = training_df.abstract_text\n",
    "\n",
    "#prepped_train, prepped_labels = \\\n",
    "#    prep_series_for_training(w2v_runtime, training_data_series, training_df.ExpansionLevel)\n",
    "\n",
    "print('Num documents: {}, num tokenized docs: {}, num labels: {}'.format(\n",
    "    training_df.shape, len(td.prepped_embedding_train), len(td.prepped_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: The present invention is directed to compounds having the formula (I):  \n",
      "                         \n",
      " \n",
      "     useful in treating inflammatory and immune diseases, in which K is O or S; Q is —C(═O)— or optionally substituted C 1-4 alkylene; Ar is optionally-substituted aryl or heteroaryl; J 1 , J 2 , J 3 , and Y are selected so that ring A is a five-to-six membered optionally-substituted cycloalkenyl or heterocyclo ring having 0 to 2 nitrogen heteroatoms; Z is N or C(R 9 ); and R 1 , R 2 , R 3  and R 9  are as defined in the specification.\n",
      "Tokenized: present invention is directed to compounds having formula i useful in treating inflammatory and immune diseases in which k is o or s q is UNK UNK UNK or optionally substituted c _NUMBER_ _NUMBER_ alkylene ar is optionally substituted aryl or heteroaryl j _NUMBER_ j _NUMBER_ j _NUMBER_ and y are selected so that ring a is a five to six membered optionally substituted cycloalkenyl or heterocyclo ring having _NUMBER_ to _NUMBER_ nitrogen heteroatoms z is n or c r _NUMBER_ and r _NUMBER_ r _NUMBER_ r _NUMBER_ and r _NUMBER_ are as defined in specification\n",
      "Integerized: [81, 45, 5, 621, 4, 305, 26, 318, 262, 442, 6, 774, 3434, 3, 3926, 1681, 6, 14, 1379, 5, 512, 11, 140, 1620, 5, 110239, 110239, 110239, 11, 764, 591, 102, 7, 7, 2705, 3341, 5, 764, 591, 1525, 11, 5534, 3102, 7, 3102, 7, 3102, 7, 3, 688, 13, 161, 66, 19, 251, 1, 5, 1, 3176, 4, 3294, 4393, 764, 591, 10828, 11, 30297, 251, 26, 7, 4, 7, 1251, 9874, 1069, 5, 192, 11, 102, 107, 7, 3, 107, 7, 107, 7, 107, 7, 3, 107, 7, 13, 23, 395, 6, 2395]\n",
      "LabelIntegerized: 1\n"
     ]
    }
   ],
   "source": [
    "show_details(td.series_text_to_embed, 0)\n",
    "#show_details(training_data_series, 4)\n",
    "#show_details(training_data_series, 400)\n",
    "#show_details(training_data_series, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "RAND_SEED=314159\n",
    "\n",
    "def randomize_training(td, percent_train):\n",
    "\n",
    "    training_data_to_shuffle = list(\n",
    "        zip(td.prepped_embedding_train, td.refs_one_hot, td.prepped_labels))\n",
    "    random.seed(RAND_SEED)\n",
    "    random.shuffle(training_data_to_shuffle)\n",
    "    train_embed_arr, refs_one_hot, label_arr = zip(*training_data_to_shuffle)\n",
    "\n",
    "    train_idx = int(len(train_embed_arr) * percent_train)\n",
    "\n",
    "    td.trainEmbedX = train_embed_arr[:train_idx]\n",
    "    td.trainRefsOneHotX = refs_one_hot[:train_idx]\n",
    "    td.testEmbedX = train_embed_arr[train_idx:]\n",
    "    td.testRefsOneHotX = refs_one_hot[train_idx:]\n",
    "\n",
    "    td.trainY = label_arr[:train_idx]\n",
    "    td.testY = label_arr[train_idx:]\n",
    "\n",
    "    return td\n",
    "\n",
    "td = randomize_training(td, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index 29.\n",
      "[1, 375, 378, 9, 4908, 10267, 69, 1, 375, 778, 9, 2595, 8804, 2, 1, 124, 75, 778, 7, 140, 269, 2, 1316, 6, 1295, 83, 8, 59, 378, 166, 10, 375, 778, 9, 365, 1, 47, 2, 777, 10267, 15, 8804, 2178, 12, 375, 778, 3, 1, 276, 166, 10, 59, 378, 79, 276, 38, 20, 6, 1, 14195, 266, 11, 8, 327, 266, 3, 79, 276, 4097, 15, 14195, 266, 4, 327, 266, 52, 59, 378, 1228, 1, 7349, 2, 56, 1858, 75, 2178, 8804, 56, 1858, 65, 262, 1, 405, 17505, 3, 1087, 8, 303, 777, 3, 79, 276, 1481, 1, 2252, 849, 875, 4, 8, 286, 25, 91, 17, 777, 10267, 493, 12, 59, 378, 52, 276, 5, 6, 327, 266, 1, 30, 3, 1, 225, 2359, 165, 272, 13, 61, 309, 3, 4229]\n",
      "a video processor for recognizing gestures including a video camera for capturing photographs of a region within camera _NUMBER_ s field of view in real time an image processor coupled with video camera for detecting a plurality of hand gestures from photographs captured by video camera and a controller coupled with image processor wherein controller can be in a dormant mode or an active mode and wherein controller transitions from dormant mode to active mode when image processor detects a progression of two states within captured photographs two states being i a closed fist and ii an open hand and wherein controller performs a programmed responsive action to an electronic device based on hand gestures detected by image processor when controller is in active mode a method and a computer readable storage medium are also described and claimed\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "['US-5454043-A', 'US-2008122796-A1', 'US-2010085274-A1', 'US-8718797-B1', 'US-2008062625-A1', 'US-5252951-A', 'US-7479949-B2', 'US-7042440-B2', 'US-2011047459-A1', 'US-6256400-B1', 'US-7340077-B2', 'US-2010162128-A1', 'US-7259747-B2', 'US-6570557-B1', 'US-2007120762-A1', 'US-7308112-B2', 'US-6545669-B1', 'US-7536032-B2', 'US-2012081303-A1', 'US-7317836-B2', 'US-7367887-B2', 'US-2010045705-A1', 'US-7555142-B2', 'US-7852262-B2', 'US-2006022955-A1', 'US-2009204925-A1', 'US-5880411-A', 'US-5767842-A', 'US-2011081889-A1', 'US-6385331-B2', 'US-2003206202-A1', 'WO-9935633-A2', 'US-2009085864-A1', 'US-2006238517-A1', 'US-2003076306-A1', 'US-2006032680-A1', 'US-6564144-B1', 'US-6266050-B1', 'US-2009228842-A1', 'US-7996045-B1', 'US-6956564-B1', 'US-2006031776-A1', 'US-5796866-A', 'US-5370697-A', 'US-6978127-B1', 'US-2010245240-A1', 'US-2011163971-A1', 'US-2006277481-A1', 'US-2001036224-A1', 'US-5889517-A', 'US-2008059814-A1', 'US-2008167809-A1', 'US-2008195637-A1', 'EP-2362636-B1', 'US-5929528-A', 'US-5892509-A', 'US-2007299387-A1', 'US-4870576-A', 'US-8699516-B2', 'US-2003167799-A1', 'US-4701273-A', 'US-2002132268-A1', 'US-5958883-A', 'US-2002179613-A1', 'US-6072809-A', 'US-2009255122-A1', 'US-1492878-A', 'US-4439809-A', 'US-5751556-A', 'WO-03085323-A1']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_idx = 0\n",
    "for row in td.trainY:\n",
    "    if td.trainY[print_idx] == 0:\n",
    "        break\n",
    "    else:\n",
    "        print_idx += 1\n",
    "\n",
    "print('Using index {}.'.format(print_idx))\n",
    "print(td.trainEmbedX[print_idx])\n",
    "print(to_text(td.trainEmbedX[print_idx]))\n",
    "print(td.trainRefsOneHotX[print_idx])\n",
    "print(ref_ids_to_text(td, td.refs_one_hot[30]))\n",
    "print(td.trainY[print_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "td.trainEmbedX, td.trainRefsOneHotX, td.testEmbedX, td.testRefsOneHotX = \\\n",
    "    np.array(td.trainEmbedX), \\\n",
    "    np.array(td.trainRefsOneHotX), \\\n",
    "    np.array(td.testEmbedX), \\\n",
    "    np.array(td.testRefsOneHotX)\n",
    "\n",
    "td.trainY, td.testY = np.array(td.trainY), np.array(td.testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training embedding data shape: (8576,), train labels shape: (8576,)\n",
      "test data shape: (2145,), test labels shape: (2145,)\n",
      "median doc length: 103\n",
      "mean doc length: 104.05771921641791\n",
      "max doc length: 384\n"
     ]
    }
   ],
   "source": [
    "print('training embedding data shape: {}, train labels shape: {}'.format(\n",
    "    td.trainEmbedX.shape, td.trainY.shape))\n",
    "print('test data shape: {}, test labels shape: {}'.format(td.testEmbedX.shape, td.testY.shape))\n",
    "\n",
    "doc_lengths = list(map(len, td.trainEmbedX))\n",
    "median_doc_length = int(np.median(doc_lengths))\n",
    "max_doc_length = np.max(doc_lengths)\n",
    "print('median doc length: {}'.format(median_doc_length))\n",
    "print('mean doc length: {}'.format(np.mean(doc_lengths)))\n",
    "print('max doc length: {}'.format(max_doc_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.tokenize(text=)\n",
    "#training_df.refs.head()\n",
    "len(td.prepped_embedding_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, BatchNormalization, ELU\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Dropout\n",
    "from keras_metrics import precision, recall, f1score\n",
    "#from keras.datasets import imdb\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_size = 128\n",
    "\n",
    "sequence_len = max_doc_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.padded_train_embed_x = sequence.pad_sequences(\n",
    "    td.trainEmbedX, maxlen=sequence_len, padding='pre', truncating='post')\n",
    "td.padded_test_embed_x = sequence.pad_sequences(\n",
    "    td.testEmbedX, maxlen=sequence_len, padding='pre', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/ipykernel/__main__.py:39: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 8576 samples, validate on 2145 samples\n",
      "Epoch 1/5\n",
      "8576/8576 [==============================] - 137s - loss: 0.0857 - acc: 0.9677 - precision: 0.9854 - recall: 0.9776 - fmeasure: 0.9799 - val_loss: 0.0314 - val_acc: 0.9911 - val_precision: 0.9924 - val_recall: 0.9979 - val_fmeasure: 0.9951\n",
      "Epoch 2/5\n",
      "8576/8576 [==============================] - 137s - loss: 0.0112 - acc: 0.9981 - precision: 0.9982 - recall: 0.9998 - fmeasure: 0.9989 - val_loss: 0.0281 - val_acc: 0.9925 - val_precision: 0.9940 - val_recall: 0.9979 - val_fmeasure: 0.9959\n",
      "Epoch 3/5\n",
      "8576/8576 [==============================] - 132s - loss: 0.0064 - acc: 0.9991 - precision: 0.9992 - recall: 0.9997 - fmeasure: 0.9995 - val_loss: 0.0269 - val_acc: 0.9935 - val_precision: 0.9955 - val_recall: 0.9974 - val_fmeasure: 0.9964\n",
      "Epoch 4/5\n",
      "8576/8576 [==============================] - 136s - loss: 0.0056 - acc: 0.9994 - precision: 0.9996 - recall: 0.9997 - fmeasure: 0.9997 - val_loss: 0.0320 - val_acc: 0.9939 - val_precision: 0.9950 - val_recall: 0.9985 - val_fmeasure: 0.9967\n",
      "Epoch 5/5\n",
      "8576/8576 [==============================] - 137s - loss: 0.0048 - acc: 0.9994 - precision: 0.9997 - recall: 0.9996 - fmeasure: 0.9997 - val_loss: 0.0332 - val_acc: 0.9935 - val_precision: 0.9945 - val_recall: 0.9985 - val_fmeasure: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fecb9928f28>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "wide = Sequential()\n",
    "wide.add(Dense(256, input_dim=td.trainRefsOneHotX.shape[1], name='refs'))\n",
    "wide.add(Dense(64, input_dim=256))\n",
    "\n",
    "deep = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(w2v_runtime.embedding_weights.shape[0],\n",
    "                            w2v_runtime.embedding_weights.shape[1],\n",
    "                            weights=[w2v_runtime.embedding_weights],\n",
    "                            #input_length=sequence_len,\n",
    "                            trainable=False,\n",
    "                            name='embed')\n",
    "deep.add(embedding_layer)\n",
    "'''\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "'''\n",
    "deep.add(LSTM(\n",
    "    lstm_size,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=False,\n",
    "    name='LSTM_1'))\n",
    "#model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, name='LSTM_2'))\n",
    "deep.add(Dense(300, activation=None))\n",
    "deep.add(Dropout(0.2))\n",
    "deep.add(BatchNormalization())\n",
    "deep.add(ELU())\n",
    "\n",
    "model = Sequential()\n",
    "#wide_input = concatenate(inputs=[wide, deep])\n",
    "#model.add(concatenate([wide, deep], axis=0))\n",
    "model.add(Merge([wide, deep], mode='concat', concat_axis=1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "#for seq, label in zip(trainX, trainY):\n",
    "#    print('fitting\\n seq: {} \\n label: {}\\n'.format(seq, label))\n",
    "#    model.train_on_batch(np.array([seq]), [label])\n",
    "\n",
    "model.fit(x={'refs_input': td.trainRefsOneHotX, 'embed_input': td.padded_train_embed_x},\n",
    "          y=td.trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(\n",
    "              {'refs_input': td.testRefsOneHotX, 'embed_input': td.padded_test_embed_x},\n",
    "              td.testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2145/2145 [==============================] - 8s     \n",
      "\n",
      "Test score: 0.0332\n",
      "Test accuracy: 0.9935\n",
      "Test p/r (f1): 0.99/1.00 (1.00)\n"
     ]
    }
   ],
   "source": [
    "score, acc, p, r, f1 = model.evaluate(\n",
    "    x={'refs_input': td.testRefsOneHotX, 'embed_input': td.padded_test_embed_x},\n",
    "    y=test_y,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "print('')\n",
    "print('Test score: {:.4f}'.format(score))\n",
    "print('Test accuracy: {:.4f}'.format(acc))\n",
    "print('Test p/r (f1): {:.2f}/{:.2f} ({:.2f})'.format(p, r, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [8, 48, 5, 90, 14, 39, 771, 1, 2506, 149, 58, 1, 18, 3471, 2191, 4, 16472, 1282, 8, 384, 2, 4482, 2, 58, 17, 2506, 3, 1, 22, 3471, 2191, 4, 3934, 1282, 8, 384, 2, 4482, 2, 58, 17, 2506, 6, 21, 288, 48, 39, 20, 82, 23, 1, 115, 9653, 240, 9, 1, 392, 58, 103],\n",
       "       [81, 45, 199, 4, 980, 470, 1475, 91, 485, 1977, 3, 1, 30, 2, 597, 197, 12, 1, 15956, 171, 401, 735, 4, 1479, 2, 387, 8, 315, 1088, 910, 2, 387, 5, 402, 1, 658, 11, 39132, 1115, 575, 2, 7, 7, 110239, 596, 5, 55, 33, 76, 3, 178, 40, 2, 387, 6, 14, 1115, 575, 576, 201, 5, 2494, 12, 53, 2, 178, 40],\n",
       "       [8, 1768, 9, 1206, 1, 144, 172, 24, 1, 76, 3, 1768, 753, 17, 76, 151, 4, 361, 882, 172, 753, 17, 1, 144, 172, 28, 1768, 126, 24, 1, 4241, 242, 40, 55, 23, 1, 242, 310, 73, 6, 582, 24573, 242, 40, 24, 1, 850, 9, 653, 172, 126, 3, 1, 310, 790, 35, 9, 2954, 367, 850, 1, 30, 9, 1937, 1768, 24, 467, 2, 2982, 76, 133, 1, 242, 40, 17, 1, 271, 35, 2, 76, 3, 757, 16, 31, 602, 2, 271, 35, 271, 35, 38, 771, 1, 1609, 363, 11, 4044, 11, 1, 271, 876, 6, 76, 242, 40, 38, 771, 1, 113, 1, 242, 371, 11, 1, 371, 679, 38, 175, 1, 3562, 97, 11, 3562, 404, 1768, 38, 20, 82, 4, 4417, 701, 202, 391, 326, 3, 683, 202, 391, 326, 23, 312, 9, 144, 280, 42, 23, 3179, 6269, 3, 3163],\n",
       "       ...,\n",
       "       [1, 30, 3, 48, 9, 7228, 1, 732, 1085, 1, 3520, 3, 3564, 3, 4700, 1, 4560, 6, 19, 732, 4, 7718, 1190, 664, 15, 3520, 1375, 831, 1, 6127, 3, 3976, 1136, 25, 14, 34545, 3, 9210, 6, 1, 274, 152, 4, 3513, 845, 1128, 154, 119, 6127, 1123, 1128, 5, 888, 12, 1, 174, 2, 2840, 69, 2323, 7228, 271, 578, 50, 1, 121, 575, 646, 1933, 6127, 1830, 4, 732, 1155, 152, 261, 3, 1155, 84, 6, 159, 2, 3809, 1085, 664, 575, 746, 6127, 3, 2781, 13, 3167, 6, 534, 10, 45, 847, 2, 3976, 1765, 3, 578, 7289, 13, 2514, 4, 1688, 321, 880],\n",
       "       [1, 562, 613, 9, 1819, 799, 11, 1819, 2308, 799, 208, 68, 1, 367, 76, 8, 613, 1043, 35, 8, 1594, 294, 182, 15, 76, 1, 18, 110, 2, 16, 31, 56, 1386, 3533, 846, 246, 16, 1, 18, 44, 147, 4, 1, 438, 51, 16, 1594, 294, 3, 16, 1, 22, 44, 4, 1043, 35, 18, 110, 65, 187, 4, 20, 8027, 970, 50, 258, 4, 438, 51, 3, 1, 22, 110, 2, 16, 31, 56, 1386, 3533, 846, 246, 16, 1, 18, 44, 4, 1594, 294, 3, 16, 1, 22, 44, 4, 1043, 35, 22, 110, 138, 65, 60, 4, 1, 438, 51, 3, 65, 320, 902, 15, 18, 110, 18, 223, 2, 16, 31, 56, 1386, 3533, 846, 2, 18, 110, 3, 1, 18, 223, 2, 16, 31, 56, 1386, 3533, 846, 2, 22, 110, 13, 6, 1, 579, 424],\n",
       "       [1, 916, 24, 1, 1161, 1762, 10, 8, 1589, 55, 87, 66, 23, 4, 92, 1, 274, 49, 8, 3037, 2814, 5, 2108, 75, 1161, 1762, 3, 1699, 60, 4, 2814, 812, 50, 1, 1431, 923, 75, 1762, 8, 189, 2023, 5, 1476, 50, 87, 3, 246, 4, 1699, 8, 190, 2, 916, 141, 24, 1, 626, 2814, 3, 1, 1105, 256, 6, 1922, 4, 1762]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow LSTM Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "vocab_size = w2v_runtime.embedding_weights.shape[0]\n",
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = w2v_runtime.embedding_weights.shape[1]\n",
    "\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    embedding = tf.get_variable(\n",
    "        name=\"embedding\",\n",
    "        shape=w2v_runtime.embedding_weights.shape,\n",
    "        initializer=tf.constant_initializer(w2v_runtime.embedding_weights),\n",
    "        trainable=False)\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.134\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.102\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.131\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.086\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.121\n",
      "Val acc: 0.884\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.083\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.064\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.211\n",
      "Epoch: 0/10 Iteration: 45 Train loss: 0.101\n",
      "Epoch: 0/10 Iteration: 50 Train loss: 0.065\n",
      "Val acc: 0.910\n",
      "Epoch: 0/10 Iteration: 55 Train loss: 0.092\n",
      "Epoch: 0/10 Iteration: 60 Train loss: 0.051\n",
      "Epoch: 0/10 Iteration: 65 Train loss: 0.053\n",
      "Epoch: 0/10 Iteration: 70 Train loss: 0.037\n",
      "Epoch: 0/10 Iteration: 75 Train loss: 0.038\n",
      "Val acc: 0.933\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.039\n",
      "Epoch: 1/10 Iteration: 85 Train loss: 0.079\n",
      "Epoch: 1/10 Iteration: 90 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 95 Train loss: 0.033\n",
      "Epoch: 1/10 Iteration: 100 Train loss: 0.042\n",
      "Val acc: 0.949\n",
      "Epoch: 1/10 Iteration: 105 Train loss: 0.057\n",
      "Epoch: 1/10 Iteration: 110 Train loss: 0.068\n",
      "Epoch: 1/10 Iteration: 115 Train loss: 0.067\n",
      "Epoch: 1/10 Iteration: 120 Train loss: 0.079\n",
      "Epoch: 1/10 Iteration: 125 Train loss: 0.049\n",
      "Val acc: 0.940\n",
      "Epoch: 1/10 Iteration: 130 Train loss: 0.049\n",
      "Epoch: 1/10 Iteration: 135 Train loss: 0.041\n",
      "Epoch: 1/10 Iteration: 140 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 145 Train loss: 0.148\n",
      "Epoch: 1/10 Iteration: 150 Train loss: 0.072\n",
      "Val acc: 0.940\n",
      "Epoch: 2/10 Iteration: 155 Train loss: 0.066\n",
      "Epoch: 2/10 Iteration: 160 Train loss: 0.056\n",
      "Epoch: 2/10 Iteration: 165 Train loss: 0.049\n",
      "Epoch: 2/10 Iteration: 170 Train loss: 0.036\n",
      "Epoch: 2/10 Iteration: 175 Train loss: 0.030\n",
      "Val acc: 0.942\n",
      "Epoch: 2/10 Iteration: 180 Train loss: 0.053\n",
      "Epoch: 2/10 Iteration: 185 Train loss: 0.038\n",
      "Epoch: 2/10 Iteration: 190 Train loss: 0.029\n",
      "Epoch: 2/10 Iteration: 195 Train loss: 0.066\n",
      "Epoch: 2/10 Iteration: 200 Train loss: 0.039\n",
      "Val acc: 0.936\n",
      "Epoch: 2/10 Iteration: 205 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 210 Train loss: 0.057\n",
      "Epoch: 2/10 Iteration: 215 Train loss: 0.103\n",
      "Epoch: 2/10 Iteration: 220 Train loss: 0.064\n",
      "Epoch: 2/10 Iteration: 225 Train loss: 0.062\n",
      "Val acc: 0.931\n",
      "Epoch: 3/10 Iteration: 230 Train loss: 0.053\n",
      "Epoch: 3/10 Iteration: 235 Train loss: 0.050\n",
      "Epoch: 3/10 Iteration: 240 Train loss: 0.057\n",
      "Epoch: 3/10 Iteration: 245 Train loss: 0.037\n",
      "Epoch: 3/10 Iteration: 250 Train loss: 0.112\n",
      "Val acc: 0.560\n",
      "Epoch: 3/10 Iteration: 255 Train loss: 0.072\n",
      "Epoch: 3/10 Iteration: 260 Train loss: 0.046\n",
      "Epoch: 3/10 Iteration: 265 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 270 Train loss: 0.053\n",
      "Epoch: 3/10 Iteration: 275 Train loss: 0.030\n",
      "Val acc: 0.947\n",
      "Epoch: 3/10 Iteration: 280 Train loss: 0.035\n",
      "Epoch: 3/10 Iteration: 285 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 290 Train loss: 0.025\n",
      "Epoch: 3/10 Iteration: 295 Train loss: 0.058\n",
      "Epoch: 3/10 Iteration: 300 Train loss: 0.053\n",
      "Val acc: 0.946\n",
      "Epoch: 4/10 Iteration: 305 Train loss: 0.022\n",
      "Epoch: 4/10 Iteration: 310 Train loss: 0.024\n",
      "Epoch: 4/10 Iteration: 315 Train loss: 0.060\n",
      "Epoch: 4/10 Iteration: 320 Train loss: 0.033\n",
      "Epoch: 4/10 Iteration: 325 Train loss: 0.068\n",
      "Val acc: 0.953\n",
      "Epoch: 4/10 Iteration: 330 Train loss: 0.034\n",
      "Epoch: 4/10 Iteration: 335 Train loss: 0.048\n",
      "Epoch: 4/10 Iteration: 340 Train loss: 0.030\n",
      "Epoch: 4/10 Iteration: 345 Train loss: 0.029\n",
      "Epoch: 4/10 Iteration: 350 Train loss: 0.017\n",
      "Val acc: 0.949\n",
      "Epoch: 4/10 Iteration: 355 Train loss: 0.046\n",
      "Epoch: 4/10 Iteration: 360 Train loss: 0.053\n",
      "Epoch: 4/10 Iteration: 365 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 370 Train loss: 0.020\n",
      "Epoch: 4/10 Iteration: 375 Train loss: 0.038\n",
      "Val acc: 0.951\n",
      "Epoch: 4/10 Iteration: 380 Train loss: 0.027\n",
      "Epoch: 5/10 Iteration: 385 Train loss: 0.026\n",
      "Epoch: 5/10 Iteration: 390 Train loss: 0.044\n",
      "Epoch: 5/10 Iteration: 395 Train loss: 0.034\n",
      "Epoch: 5/10 Iteration: 400 Train loss: 0.038\n",
      "Val acc: 0.950\n",
      "Epoch: 5/10 Iteration: 405 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 410 Train loss: 0.027\n",
      "Epoch: 5/10 Iteration: 415 Train loss: 0.034\n",
      "Epoch: 5/10 Iteration: 420 Train loss: 0.051\n",
      "Epoch: 5/10 Iteration: 425 Train loss: 0.048\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 430 Train loss: 0.025\n",
      "Epoch: 5/10 Iteration: 435 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 440 Train loss: 0.025\n",
      "Epoch: 5/10 Iteration: 445 Train loss: 0.052\n",
      "Epoch: 5/10 Iteration: 450 Train loss: 0.031\n",
      "Val acc: 0.944\n",
      "Epoch: 5/10 Iteration: 455 Train loss: 0.033\n",
      "Epoch: 6/10 Iteration: 460 Train loss: 0.019\n",
      "Epoch: 6/10 Iteration: 465 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 470 Train loss: 0.047\n",
      "Epoch: 6/10 Iteration: 475 Train loss: 0.021\n",
      "Val acc: 0.950\n",
      "Epoch: 6/10 Iteration: 480 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 485 Train loss: 0.053\n",
      "Epoch: 6/10 Iteration: 490 Train loss: 0.042\n",
      "Epoch: 6/10 Iteration: 495 Train loss: 0.055\n",
      "Epoch: 6/10 Iteration: 500 Train loss: 0.028\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 505 Train loss: 0.032\n",
      "Epoch: 6/10 Iteration: 510 Train loss: 0.027\n",
      "Epoch: 6/10 Iteration: 515 Train loss: 0.031\n",
      "Epoch: 6/10 Iteration: 520 Train loss: 0.045\n",
      "Epoch: 6/10 Iteration: 525 Train loss: 0.035\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 530 Train loss: 0.041\n",
      "Epoch: 7/10 Iteration: 535 Train loss: 0.046\n",
      "Epoch: 7/10 Iteration: 540 Train loss: 0.026\n",
      "Epoch: 7/10 Iteration: 545 Train loss: 0.033\n",
      "Epoch: 7/10 Iteration: 550 Train loss: 0.029\n",
      "Val acc: 0.950\n",
      "Epoch: 7/10 Iteration: 555 Train loss: 0.027\n",
      "Epoch: 7/10 Iteration: 560 Train loss: 0.044\n",
      "Epoch: 7/10 Iteration: 565 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 570 Train loss: 0.023\n",
      "Epoch: 7/10 Iteration: 575 Train loss: 0.041\n",
      "Val acc: 0.951\n",
      "Epoch: 7/10 Iteration: 580 Train loss: 0.020\n",
      "Epoch: 7/10 Iteration: 585 Train loss: 0.013\n",
      "Epoch: 7/10 Iteration: 590 Train loss: 0.023\n",
      "Epoch: 7/10 Iteration: 595 Train loss: 0.017\n",
      "Epoch: 7/10 Iteration: 600 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 7/10 Iteration: 605 Train loss: 0.050\n",
      "Epoch: 8/10 Iteration: 610 Train loss: 0.025\n",
      "Epoch: 8/10 Iteration: 615 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 620 Train loss: 0.044\n",
      "Epoch: 8/10 Iteration: 625 Train loss: 0.027\n",
      "Val acc: 0.950\n",
      "Epoch: 8/10 Iteration: 630 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 635 Train loss: 0.064\n",
      "Epoch: 8/10 Iteration: 640 Train loss: 0.027\n",
      "Epoch: 8/10 Iteration: 645 Train loss: 0.034\n",
      "Epoch: 8/10 Iteration: 650 Train loss: 0.034\n",
      "Val acc: 0.954\n",
      "Epoch: 8/10 Iteration: 655 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 660 Train loss: 0.028\n",
      "Epoch: 8/10 Iteration: 665 Train loss: 0.021\n",
      "Epoch: 8/10 Iteration: 670 Train loss: 0.015\n",
      "Epoch: 8/10 Iteration: 675 Train loss: 0.043\n",
      "Val acc: 0.950\n",
      "Epoch: 8/10 Iteration: 680 Train loss: 0.040\n",
      "Epoch: 9/10 Iteration: 685 Train loss: 0.053\n",
      "Epoch: 9/10 Iteration: 690 Train loss: 0.045\n",
      "Epoch: 9/10 Iteration: 695 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 700 Train loss: 0.033\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 705 Train loss: 0.066\n",
      "Epoch: 9/10 Iteration: 710 Train loss: 0.027\n",
      "Epoch: 9/10 Iteration: 715 Train loss: 0.044\n",
      "Epoch: 9/10 Iteration: 720 Train loss: 0.031\n",
      "Epoch: 9/10 Iteration: 725 Train loss: 0.021\n",
      "Val acc: 0.950\n",
      "Epoch: 9/10 Iteration: 730 Train loss: 0.019\n",
      "Epoch: 9/10 Iteration: 735 Train loss: 0.042\n",
      "Epoch: 9/10 Iteration: 740 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 745 Train loss: 0.033\n",
      "Epoch: 9/10 Iteration: 750 Train loss: 0.018\n",
      "Val acc: 0.948\n",
      "Epoch: 9/10 Iteration: 755 Train loss: 0.043\n",
      "Epoch: 9/10 Iteration: 760 Train loss: 0.034\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph, config=GPU_MEM_CONFIG) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(test_x, test_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(l1_x, l1_y, batch_size), 1): #can also use test_x, test_y\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference! (Ignore everything below here for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import expansion\n",
    "\n",
    "importlib.reload(expansion)\n",
    "expander = expansion.PatentLandscapeExpander(\n",
    "    seed_file,\n",
    "    bq_project=bq_project,\n",
    "    patent_dataset=patent_dataset,\n",
    "    num_antiseed=num_anti_seed_patents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe with cols Index(['publication_number'], dtype='object'), shape (133137, 1), to patents._tmp_training\n",
      "Completed loading temp table.\n",
      "Loading patent texts from provided publication numbers.\n"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: apiLimitExceeded, Message: API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mjobId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_reference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jobId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                     pageToken=page_token).execute()\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/oauth2client/util.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://www.googleapis.com/bigquery/v2/projects/patent-landscape-165715/queries/job_Fp1OMsWBelejjyoKkxXfmQPYNs4?alt=json&pageToken=BGEWZFUNLQAQAAASA4EAAEEAQCAAKGQIBDF6UAQQUCGQMIFQVYKQ%3D%3D%3D%3D returned \"API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f64ee6646dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#training_df[training_df.abstract_text.str.contains('machine')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ml1_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_training_data_from_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_patents_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'publication_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0ml1_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Seed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ml1_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/src/models/google-patent-models/expansion.py\u001b[0m in \u001b[0;36mload_training_data_from_pubs\u001b[0;34m(self, training_publications_df)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             configuration = {'query': {'useQueryCache': True, 'allowLargeResults': False}})\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_data_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, dialect, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, verbose, private_key, dialect, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m                              \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                              dialect=dialect)\n\u001b[0;32m--> 722\u001b[0;31m     \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0mdataframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m                     pageToken=page_token).execute()\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_row\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_rows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 raise GenericGBQException(\n\u001b[0;32m--> 352\u001b[0;31m                     \"Reason: {0}, Message: {1}\".format(reason, message))\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGenericGBQException\u001b[0m: Reason: apiLimitExceeded, Message: API limit exceeded: Unable to return a row that exceeds the API limits. To retrieve the row, export the table."
     ]
    }
   ],
   "source": [
    "\n",
    "#training_df[training_df.abstract_text.str.contains('learn') & training_df.abstract_text.str.contains('machine')]\n",
    "\n",
    "#training_df[training_df.abstract_text.str.contains('machine')]\n",
    "l1_texts = expander.load_training_data_from_pubs(l1_patents_df[['publication_number']])\n",
    "l1_texts['label'] = 'Seed'\n",
    "l1_texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def binary_prediction_idx(score):\n",
    "    if score < .5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def predictions_for_report(patent_model, validX, validY):\n",
    "    target_names = ['seed', 'antiseed']\n",
    "\n",
    "    prediction_scores = patent_model.predict(validX)\n",
    "    predictions = []\n",
    "    actual_y = []\n",
    "\n",
    "    for idx in range(0, len(prediction_scores)):\n",
    "        prediction = prediction_scores[idx]\n",
    "        actual = validY[idx]\n",
    "        predictions.append(binary_prediction_idx(prediction[0]))\n",
    "        actual_y.append(actual)\n",
    "\n",
    "    return predictions, actual_y, target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping series (tokenization etc.) for training.\n",
      "Padding sequences.\n",
      "Making predictions\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       seed       0.31      1.00      0.47      3070\n",
      "   antiseed       1.00      0.00      0.00      6930\n",
      "\n",
      "avg / total       0.79      0.31      0.14     10000\n",
      "\n",
      "[[3070    0]\n",
      " [6929    1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "l1_texts_subset = l1_texts.abstract_text[:10000]\n",
    "l1_texts_subset.reset_index(drop=True, inplace=True)\n",
    "l1_texts_subset.loc[len(l1_texts_subset)-1] = 'this abstract is about something entirely boring'\n",
    "\n",
    "l1_labels_subset = l1_texts.label[:10000]\n",
    "l1_labels_subset.reset_index(drop=True, inplace=True)\n",
    "l1_labels_subset.loc[len(l1_texts_subset)-1] = 'AntiSeed'\n",
    "\n",
    "print('Prepping series (tokenization etc.) for training.')\n",
    "l1_x, l1_y = prep_series_for_training(w2v_runtime, l1_texts_subset, l1_labels_subset)\n",
    "l1_x = np.array(l1_x)\n",
    "l1_y = np.array(l1_y)\n",
    "\n",
    "print('Padding sequences.')\n",
    "# Convert text idx into padded sequences\n",
    "l1_x = sequence.pad_sequences(\n",
    "        l1_x, maxlen=sequence_len, padding='pre', truncating='post')\n",
    "# Converting labels to binary vectors\n",
    "#l1_y = to_categorical(l1_y, nb_classes=2)\n",
    "\n",
    "print('Making predictions')\n",
    "l1_preds, actual_l1_y, target_names = predictions_for_report(model, l1_x, l1_y)\n",
    "\n",
    "cr = classification_report(l1_preds, actual_l1_y, target_names=target_names)\n",
    "cm = confusion_matrix(l1_preds, actual_l1_y)\n",
    "print(cr)\n",
    "print(cm)\n",
    "#l1_texts_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PredictionIdx Prediction\n",
       "0                 1   antiseed\n",
       "1                 1   antiseed\n",
       "2                 1   antiseed\n",
       "3                 0       seed\n",
       "4                 1   antiseed\n",
       "5                 1   antiseed\n",
       "6                 1   antiseed\n",
       "7                 1   antiseed\n",
       "8                 1   antiseed\n",
       "9                 1   antiseed\n",
       "10                1   antiseed\n",
       "11                1   antiseed\n",
       "12                1   antiseed\n",
       "13                1   antiseed\n",
       "14                1   antiseed\n",
       "15                0       seed\n",
       "16                1   antiseed\n",
       "17                1   antiseed\n",
       "18                1   antiseed\n",
       "19                1   antiseed\n",
       "20                1   antiseed\n",
       "21                0       seed\n",
       "22                1   antiseed\n",
       "23                0       seed\n",
       "24                0       seed\n",
       "25                1   antiseed\n",
       "26                1   antiseed\n",
       "27                0       seed\n",
       "28                0       seed\n",
       "29                1   antiseed\n",
       "...             ...        ...\n",
       "9970              1   antiseed\n",
       "9971              1   antiseed\n",
       "9972              1   antiseed\n",
       "9973              1   antiseed\n",
       "9974              0       seed\n",
       "9975              1   antiseed\n",
       "9976              1   antiseed\n",
       "9977              1   antiseed\n",
       "9978              0       seed\n",
       "9979              1   antiseed\n",
       "9980              1   antiseed\n",
       "9981              1   antiseed\n",
       "9982              0       seed\n",
       "9983              1   antiseed\n",
       "9984              0       seed\n",
       "9985              1   antiseed\n",
       "9986              1   antiseed\n",
       "9987              1   antiseed\n",
       "9988              1   antiseed\n",
       "9989              1   antiseed\n",
       "9990              1   antiseed\n",
       "9991              1   antiseed\n",
       "9992              1   antiseed\n",
       "9993              1   antiseed\n",
       "9994              1   antiseed\n",
       "9995              0       seed\n",
       "9996              1   antiseed\n",
       "9997              1   antiseed\n",
       "9998              0       seed\n",
       "9999              1   antiseed\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(l1_preds)\n",
    "l1_preds_df = pd.DataFrame(l1_preds, columns=['PredictionIdx'])\n",
    "l1_preds_df['Prediction'] = l1_preds_df.PredictionIdx.apply(label_id_to_text)\n",
    "l1_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The present invention relates to a method for detecting fetal Down syndrome (Trisomy 21), trisomy 13, trisomy 18 and other chromosomal anomalies during prenatal screening by analyzing a dried blood sample from a pregnant woman. More particularly the present invention relates to a method for improving detection efficiency in screening for the anomalies by measuring the amount of the free beta human chorionic gonadotropin (HCG) and nicked or fragmented or aberrant forms of free beta (HCG), all of which are referenced throughout this application as free beta (HCG) in dried blood samples from pregnant women.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_texts_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd3d8c6f978>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGaCAYAAAAl57hmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd28aebbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm_df = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_prediction(patent_model, text, expected_label, max_seq_len, mask_idx=0):\n",
    "    inf_series = pd.Series([text])\n",
    "    lab_series = pd.Series([label])\n",
    "\n",
    "    prepped_train, prepped_labels = \\\n",
    "        prep_series_for_training(\n",
    "            w2v_runtime=w2v_runtime,\n",
    "            labels_series=lab_series,\n",
    "            raw_series_text=inf_series)\n",
    "\n",
    "    # make sure we have the correct sequence length\n",
    "    #infX = pad_sequences(prepped_train, maxlen=max_seq_len, value=mask_idx)\n",
    "    infX = sequence.pad_sequences(\n",
    "        prepped_train, maxlen=sequence_len, padding='pre', truncating='post')\n",
    "    #prepped_train = np.array(prepped_train)\n",
    "\n",
    "    # actually make prediction\n",
    "    prediction = patent_model.predict(infX)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(filepath='checkpoints/keras_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: three dimensional mapping\n",
      "Prediction: [[ 0.98487407]]\n",
      "Predicted Label: antiseed, Actual Label: Seed\n"
     ]
    }
   ],
   "source": [
    "text = 'a really boring title that is super long and has nothing to do with the topic that im interested in. but has a bit of stuff about artificial neural networks and things i haven\\'t heard of?'\n",
    "text = 'three dimensional mapping'\n",
    "#text = l1_texts_subset[10]\n",
    "label = 'Seed'\n",
    "prediction = make_prediction(model, text, expected_label=label, max_seq_len=sequence_len)\n",
    "print('Text: {}'.format(text))\n",
    "#print('Tokenized/Integerized: {}'.format(prepped_train))\n",
    "#print('Padded: {}'.format(infX))\n",
    "print('Prediction: {}'.format(prediction))\n",
    "print('Predicted Label: {}, Actual Label: {}'.format(label_id_to_text(binary_prediction_idx(prediction)), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#oldidx = print_idx\n",
    "#print_idx = oldidx\n",
    "#print(model.predict([trainX[print_idx]]))\n",
    "#tensor_label_to_text(trainY[print_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pl35]",
   "language": "python",
   "name": "conda-env-pl35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add header/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "GPU_MEM_CONFIG = tf.ConfigProto(gpu_options={'allow_growth': True})\n",
    "seed_file = 'seeds/dnn.csv'\n",
    "# BigQuery must be enabled for this project\n",
    "bq_project = 'patent-landscape-165715'\n",
    "patent_dataset = 'patents-public-data:patents.publications_latest'\n",
    "num_anti_seed_patents = 15000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use models/5.9m directory to load/persist model information.\n",
      "INFO:tensorflow:Restoring parameters from models/5.9m/checkpoints/5.9m_abstracts.ckpt-1325000\n"
     ]
    }
   ],
   "source": [
    "from expansion import PatentLandscapeExpander\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "expander = PatentLandscapeExpander(\n",
    "    seed_file,\n",
    "    bq_project=bq_project,\n",
    "    patent_dataset=patent_dataset,\n",
    "    num_antiseed=num_anti_seed_patents)\n",
    "\n",
    "\n",
    "word2vec5_9m = Word2Vec('5.9m')\n",
    "w2v_runtime = word2vec5_9m.restore_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for all US CPC Counts\n",
      "Querying for Seed Set CPC Counts\n",
      "Querying to find total number of US patents\n",
      "Got 16587 relevant seed refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (16587, 1), to patents._l1_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L1 expansion: (16457, 3)\n",
      "Got 137610 relevant L1->L2 refs\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (137610, 1), to patents._l2_tmp\n",
      "Completed loading temp table.\n",
      "Shape of L2 expansion: (130899, 3)\n",
      "Size of union of [Seed, L1, and L2]: 136087\n",
      "Loading dataframe with cols Index(['pub_num'], dtype='object'), shape (136087, 1), to patents.antiseed_tmp\n",
      "Completed loading temp table.\n",
      "Loading training data text from (16553, 2) publication numbers\n",
      "Loading dataframe with cols Index(['publication_number'], dtype='object'), shape (16553, 1), to patents._tmp_training\n",
      "Completed loading temp table.\n",
      "Loading patent texts from provided publication numbers.\n",
      "Merging labels into training data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_num</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>family_id</th>\n",
       "      <th>priority_date</th>\n",
       "      <th>title_lang</th>\n",
       "      <th>abstract_lang</th>\n",
       "      <th>claims_lang</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>title_text</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>description_text</th>\n",
       "      <th>ExpansionLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014171787</td>\n",
       "      <td>US-2014171787-A1</td>\n",
       "      <td>50884025</td>\n",
       "      <td>20121207</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Surgical procedure management systems and methods</td>\n",
       "      <td>Systems and methods for tracking a surgical to...</td>\n",
       "      <td>What is claimed is: \\n       \\n         1 . A ...</td>\n",
       "      <td>CROSS-REFERENCE TO RELATED APPLICATION \\n     ...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014290597</td>\n",
       "      <td>US-2014290597-A1</td>\n",
       "      <td>51520027</td>\n",
       "      <td>20130328</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Method for operating a direct fuel injector</td>\n",
       "      <td>A method, comprising: operating an engine cyli...</td>\n",
       "      <td>1 . A method, comprising:\\n operating an engin...</td>\n",
       "      <td>BACKGROUND AND SUMMARY \\n       [0001]    Engi...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016044408</td>\n",
       "      <td>US-2016044408-A1</td>\n",
       "      <td>55268450</td>\n",
       "      <td>20140805</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Boundary microphone and boundary microphone ad...</td>\n",
       "      <td>A boundary microphone that can reduce the chan...</td>\n",
       "      <td>What is claimed is: \\n       \\n         1 . A ...</td>\n",
       "      <td>BACKGROUND OF  THE  INVENTION \\n       [0001] ...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016183793</td>\n",
       "      <td>US-2016183793-A1</td>\n",
       "      <td>56162852</td>\n",
       "      <td>20141225</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Wireless Catheter System for Cardiac Electroph...</td>\n",
       "      <td>A Wireless Catheter Module (WCM) is adapted fo...</td>\n",
       "      <td>1 . A wireless catheter data communication sys...</td>\n",
       "      <td>BACKGROUND \\n       [0001]    1. Field of the ...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016341326</td>\n",
       "      <td>US-2016341326-A1</td>\n",
       "      <td>57325274</td>\n",
       "      <td>20150520</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Liquid container leveler</td>\n",
       "      <td>An apparatus is provided to allow over filling...</td>\n",
       "      <td>What is claimed is: \\n       \\n         1 . An...</td>\n",
       "      <td>FIELD OF THE INVENTION \\n       [0001]    The ...</td>\n",
       "      <td>AntiSeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_num publication_number family_id  priority_date title_lang  \\\n",
       "0  2014171787   US-2014171787-A1  50884025       20121207         en   \n",
       "1  2014290597   US-2014290597-A1  51520027       20130328         en   \n",
       "2  2016044408   US-2016044408-A1  55268450       20140805         en   \n",
       "3  2016183793   US-2016183793-A1  56162852       20141225         en   \n",
       "4  2016341326   US-2016341326-A1  57325274       20150520         en   \n",
       "\n",
       "  abstract_lang claims_lang description_lang  \\\n",
       "0            en          en               en   \n",
       "1            en          en               en   \n",
       "2            en          en               en   \n",
       "3            en          en               en   \n",
       "4            en          en               en   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  Surgical procedure management systems and methods   \n",
       "1        Method for operating a direct fuel injector   \n",
       "2  Boundary microphone and boundary microphone ad...   \n",
       "3  Wireless Catheter System for Cardiac Electroph...   \n",
       "4                           Liquid container leveler   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  Systems and methods for tracking a surgical to...   \n",
       "1  A method, comprising: operating an engine cyli...   \n",
       "2  A boundary microphone that can reduce the chan...   \n",
       "3  A Wireless Catheter Module (WCM) is adapted fo...   \n",
       "4  An apparatus is provided to allow over filling...   \n",
       "\n",
       "                                         claims_text  \\\n",
       "0  What is claimed is: \\n       \\n         1 . A ...   \n",
       "1  1 . A method, comprising:\\n operating an engin...   \n",
       "2  What is claimed is: \\n       \\n         1 . A ...   \n",
       "3  1 . A wireless catheter data communication sys...   \n",
       "4  What is claimed is: \\n       \\n         1 . An...   \n",
       "\n",
       "                                    description_text ExpansionLevel  \n",
       "0  CROSS-REFERENCE TO RELATED APPLICATION \\n     ...       AntiSeed  \n",
       "1  BACKGROUND AND SUMMARY \\n       [0001]    Engi...       AntiSeed  \n",
       "2  BACKGROUND OF  THE  INVENTION \\n       [0001] ...       AntiSeed  \n",
       "3  BACKGROUND \\n       [0001]    1. Field of the ...       AntiSeed  \n",
       "4  FIELD OF THE INVENTION \\n       [0001]    The ...       AntiSeed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_full_df, seed_patents_df, l1_patents_df, l2_patents_df, anti_seed_patents = \\\n",
    "    expander.derive_training_data_from_seeds(seed_file)\n",
    "training_data_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'distance': 0.32779269572446956, 'index': 17523, 'word': 'movies'},\n",
       " {'distance': 0.4143014460916512, 'index': 16271, 'word': 'theater'},\n",
       " {'distance': 0.45228573856143639, 'index': 8336, 'word': 'photography'},\n",
       " {'distance': 0.46586198646042409, 'index': 14299, 'word': 'titles'},\n",
       " {'distance': 0.48331811403700065, 'index': 7448, 'word': 'title'},\n",
       " {'distance': 0.48399591970497613, 'index': 27391, 'word': 'filmed'},\n",
       " {'distance': 0.49104971181473067, 'index': 3912, 'word': 'viewer'},\n",
       " {'distance': 0.49438433435760043, 'index': 24883, 'word': 'cinema'},\n",
       " {'distance': 0.50278364123260055, 'index': 1859, 'word': 'still'},\n",
       " {'distance': 0.51047964928984513, 'index': 2744, 'word': 'playback'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_embedding = w2v_runtime.load_embedding('test')\n",
    "w2v_runtime.find_similar('movie', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(110240, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(w2v_runtime.embedding_weights))\n",
    "w2v_runtime.embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = training_data_full_df[\n",
    "    ['publication_number', 'title_text', 'abstract_text', 'claims_text', 'description_text', 'ExpansionLevel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication_number    1484\n",
      "title_text            1484\n",
      "abstract_text         1484\n",
      "claims_text           1484\n",
      "description_text      1484\n",
      "ExpansionLevel        1484\n",
      "dtype: int64\n",
      "publication_number    10617\n",
      "title_text            10617\n",
      "abstract_text         10617\n",
      "claims_text           10617\n",
      "description_text      10617\n",
      "ExpansionLevel        10617\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(training_df[training_df.ExpansionLevel == 'Seed'].count())\n",
    "print(training_df[training_df.ExpansionLevel == 'AntiSeed'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tokenizer\n",
    "tokenizer = tokenizer.TextTokenizer()\n",
    "\n",
    "def label_text_to_id(label_name):\n",
    "    if label_name == 'antiseed':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def label_id_to_text(label_idx):\n",
    "    if label_idx == 1:\n",
    "        return 'antiseed'\n",
    "    else:\n",
    "        return 'seed'\n",
    "\n",
    "def tensor_label_to_text(tensor_text):\n",
    "    # if element 0 is '1', it means that this is the\n",
    "    # zeroth label index\n",
    "    if tensor_text[0] == 1.0:\n",
    "        return label_id_to_text(0)\n",
    "    return label_id_to_text(1)\n",
    "\n",
    "def prediction_to_label(predict_tensor):\n",
    "    return label_id_to_text(prediction_to_idx(predict_tensor))\n",
    "\n",
    "def prediction_to_idx(predict_tensor):\n",
    "    # if element 0 is greater than element 1, it's the label\n",
    "    if predict_tensor[0] > predict_tensor[1]:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def to_text(integerized):\n",
    "    words = []\n",
    "    for word_int in integerized:\n",
    "        words.append(w2v_runtime.index_to_word[word_int])\n",
    "    return ' '.join(words)\n",
    "\n",
    "def show_details(training_data_series, idx):\n",
    "    print('\\nOriginal: {}\\nTokenized: {}\\nIntegerized: {}\\nLabelIntegerized: {}'.format(\n",
    "        training_data_series[idx],\n",
    "        to_text(prepped_train[idx]),\n",
    "        prepped_train[idx],\n",
    "        prepped_labels[idx]))\n",
    "\n",
    "def prep_series_for_training(w2v_runtime, raw_series_text, labels_series):\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize_series(raw_series_text)\n",
    "    word_to_index_dict = w2v_runtime.word_to_index\n",
    "    tokenized_indexed_titles = []\n",
    "    labels_indexed = []\n",
    "\n",
    "    for idx in range(0, len(tokenized_text)):\n",
    "        title = tokenized_text[idx]\n",
    "        label = labels_series[idx]\n",
    "        title_word_indexes = []\n",
    "        for word in title:\n",
    "            if word in word_to_index_dict:\n",
    "                word_idx = word_to_index_dict[word]\n",
    "            else:\n",
    "                word_idx = word_to_index_dict['UNK']\n",
    "            # this skips 'the' so it can be used for dynamic rnn\n",
    "            if word_idx > 0:\n",
    "                title_word_indexes.append(word_idx)\n",
    "\n",
    "        tokenized_indexed_titles.append(title_word_indexes)\n",
    "        tokenized_label = tokenizer.tokenize(label)[0]\n",
    "        label_idx = label_text_to_id(tokenized_label)\n",
    "        labels_indexed.append(label_idx)\n",
    "\n",
    "    return tokenized_indexed_titles, labels_indexed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: (12101, 6), num tokenized docs: 12101, num labels: 12101\n"
     ]
    }
   ],
   "source": [
    "training_data_series = training_df.abstract_text\n",
    "\n",
    "prepped_train, prepped_labels = \\\n",
    "    prep_series_for_training(w2v_runtime, training_data_series, training_df.ExpansionLevel)\n",
    "\n",
    "print('Num documents: {}, num tokenized docs: {}, num labels: {}'.format(\n",
    "    training_df.shape, len(prepped_train), len(prepped_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: Systems and methods for tracking a surgical tool. Exemplary embodiments can comprise a surgical port, a tracking element configured for coupling to a surgical tool, and a camera mounted to a proximal end of the surgical port and configured to capture image data associated with the tracking element.\n",
      "Tokenized: systems and methods for tracking a surgical tool exemplary embodiments can comprise a surgical port a tracking element configured for coupling to a surgical tool and a camera mounted to a proximal end of surgical port and configured to capture image data associated with tracking element\n",
      "Integerized: [326, 3, 155, 9, 1605, 1, 1782, 344, 2129, 418, 38, 771, 1, 1782, 421, 1, 1605, 73, 151, 9, 414, 4, 1, 1782, 344, 3, 1, 778, 122, 4, 1, 1401, 44, 2, 1782, 421, 3, 151, 4, 2031, 59, 29, 170, 10, 1605, 73]\n",
      "LabelIntegerized: 1\n"
     ]
    }
   ],
   "source": [
    "show_details(training_data_series, 0)\n",
    "#show_details(training_data_series, 4)\n",
    "#show_details(training_data_series, 400)\n",
    "#show_details(training_data_series, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomize_training(train_arr, label_arr, percent_train):\n",
    "    training_data_to_shuffle = list(zip(train_arr, label_arr))\n",
    "    random.shuffle(training_data_to_shuffle)\n",
    "    train_arr, label_arr = zip(*training_data_to_shuffle)\n",
    "\n",
    "    train_idx = int(len(train_arr) * percent_train)\n",
    "\n",
    "    trainX = train_arr[:train_idx]\n",
    "    testX = train_arr[train_idx:]\n",
    "\n",
    "    trainY = label_arr[:train_idx]\n",
    "    testY = label_arr[train_idx:]\n",
    "\n",
    "    return trainX, testX, trainY, testY\n",
    "\n",
    "trainX, testX, trainY, testY = randomize_training(prepped_train, prepped_labels, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index 1.\n",
      "[1, 27, 3, 30, 9, 364, 7666, 1107, 12, 196, 676, 2409, 13, 90, 9521, 309, 21, 190, 5, 1, 30, 78, 2508, 88, 17, 8, 348, 72, 27, 2508, 119, 17, 348, 72, 27, 364, 1, 3233, 119, 91, 245, 6151, 88, 1536, 3233, 119, 10, 6151, 119, 3, 364, 1, 3770, 119, 172, 89, 1, 47, 2, 3253, 347, 52, 3233, 119, 10847, 10, 6151, 119]\n",
      "a system and method for determining harmonics caused by non linear loads are disclosed briefly described one embodiment is a method comprising metering voltage on an electric power system metering current on electric power system determining a predicted current based upon metered voltage comparing predicted current with metered current and determining a harmonic current component using a plurality of weights determined when predicted current converges with metered current\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_idx = 0\n",
    "for row in trainY:\n",
    "    if trainY[print_idx] == 0:\n",
    "        break\n",
    "    else:\n",
    "        print_idx += 1\n",
    "\n",
    "print('Using index {}.'.format(print_idx))\n",
    "print(trainX[print_idx])\n",
    "print(to_text(trainX[print_idx]))\n",
    "print(trainY[print_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainX, testX = np.array(trainX), np.array(testX)\n",
    "trainY, testY = np.array(trainY), np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (9680,), train labels shape: (9680,)\n",
      "test data shape: (2421,), test labels shape: (2421,)\n",
      "median doc length: 104\n",
      "mean doc length: 105.51673553719009\n",
      "max doc length: 819\n"
     ]
    }
   ],
   "source": [
    "print('training data shape: {}, train labels shape: {}'.format(trainX.shape, trainY.shape))\n",
    "print('test data shape: {}, test labels shape: {}'.format(testX.shape, testY.shape))\n",
    "\n",
    "doc_lengths = list(map(len, trainX))\n",
    "median_doc_length = int(np.median(doc_lengths))\n",
    "print('median doc length: {}'.format(median_doc_length))\n",
    "print('mean doc length: {}'.format(np.mean(doc_lengths)))\n",
    "print('max doc length: {}'.format(np.max(doc_lengths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "\n",
    "sequence_len = median_doc_length + 10\n",
    "\n",
    "unk_idx = w2v_runtime.word_to_index['UNK']\n",
    "mask_idx = unk_idx#0 # 'the'\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=sequence_len, value=mask_idx)\n",
    "testX = pad_sequences(testX, maxlen=sequence_len, value=mask_idx)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     1,     27,      3,     30,      9,    364,   7666,   1107,\n",
       "           12,    196,    676,   2409,     13,     90,   9521,    309,\n",
       "           21,    190,      5,      1,     30,     78,   2508,     88,\n",
       "           17,      8,    348,     72,     27,   2508,    119,     17,\n",
       "          348,     72,     27,    364,      1,   3233,    119,     91,\n",
       "          245,   6151,     88,   1536,   3233,    119,     10,   6151,\n",
       "          119,      3,    364,      1,   3770,    119,    172,     89,\n",
       "            1,     47,      2,   3253,    347,     52,   3233,    119,\n",
       "        10847,     10,   6151,    119, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239, 110239, 110239, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239, 110239, 110239, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239, 110239, 110239, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239, 110239, 110239, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239, 110239, 110239, 110239, 110239, 110239, 110239,\n",
       "       110239, 110239], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[print_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02672337 -0.0415425   0.04715231 -0.02182292 -0.04397739  0.00257788\n",
      "  0.01632516  0.00512346  0.02428765  0.03930404]\n",
      "[-0.08050653 -0.1251505   0.14205055 -0.0657435  -0.13248584  0.00776609\n",
      "  0.04918102  0.01543488  0.07316872  0.11840695]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_runtime.normed_embedding_weights[0][:10])\n",
    "print(w2v_runtime.embedding_weights[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_landscape_lstm(\n",
    "    sequence_length,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    dropout=0.8,\n",
    "    learn_rate=0.001,\n",
    "    tboard_verbosity=0,\n",
    "    pretrained_embedding_weights=None):\n",
    "\n",
    "    if pretrained_embedding_weights is None:\n",
    "        raise Exception('must specify pretrained embedding weights')\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, sequence_length])\n",
    "    print('Shape of input tensor: {}'.format(net.shape))\n",
    "    # TODO: consider not requiring pretrained embedding weights\n",
    "    net = tflearn.embedding(\n",
    "        net,\n",
    "        input_dim=int(vocab_size),\n",
    "        output_dim=int(embedding_size),\n",
    "        trainable=False,\n",
    "        name=\"EmbeddingLayer\")\n",
    "    #net = tflearn.embedding(net, input_dim=vocab_size, output_dim=128)\n",
    "    print('Shape of embedding tensor: {}'.format(net.shape))\n",
    "    net = tflearn.lstm(\n",
    "        incoming=net,\n",
    "        n_units=int(embedding_size),\n",
    "        dropout=dropout,\n",
    "        dynamic=True,\n",
    "        return_seq=False)\n",
    "    print('Shape of 1st LSTM tensor: {}'.format(net.shape))\n",
    "\n",
    "    net = tflearn.fully_connected(net, int(embedding_size/2), activation='elu')\n",
    "    print('Shape of 1st FC tensor: {}'.format(net.shape))\n",
    "\n",
    "    net = tflearn.layers.normalization.batch_normalization(net)\n",
    "    print('Shape of BatchNorm tensor: {}'.format(net.shape))\n",
    "\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    print('Shape of output FC tensor: {}'.format(net.shape))\n",
    "\n",
    "    net = tflearn.regression(net,\n",
    "                             optimizer='adam',\n",
    "                             learning_rate=learn_rate,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    print('Shape of regression tensor: {}'.format(net.shape))\n",
    "    print('Created network.')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=tboard_verbosity)\n",
    "    print('Created model graph.')\n",
    "\n",
    "    # Retrieve embedding layer weights (only a single weight matrix, so index is 0)\n",
    "    print('Inserting word2vec word embeddings')\n",
    "    embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "    # Assign your own weights (for example, a numpy array [input_dim, output_dim])\n",
    "    model.set_weights(embeddingWeights, pretrained_embedding_weights)\n",
    "    print('Model ready for training.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 110240\n",
      "Embedding size: 300\n",
      "Shape of input tensor: (?, 114)\n",
      "Shape of embedding tensor: (?, 114, 300)\n",
      "Shape of 1st LSTM tensor: (?, 300)\n",
      "Shape of 1st FC tensor: (?, 150)\n",
      "Shape of BatchNorm tensor: (?, 150)\n",
      "Shape of output FC tensor: (?, 2)\n",
      "Shape of regression tensor: (?, 2)\n",
      "Created network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model graph.\n",
      "Inserting word2vec word embeddings\n",
      "Model ready for training.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = w2v_runtime.embedding_weights.shape[0]\n",
    "embedding_size = w2v_runtime.embedding_weights.shape[1]\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "print('Embedding size: {}'.format(embedding_size))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = build_landscape_lstm(\n",
    "        sequence_length=sequence_len,\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=embedding_size,\n",
    "        dropout=0.75,\n",
    "        learn_rate=0.01,\n",
    "        tboard_verbosity=3,\n",
    "        pretrained_embedding_weights=w2v_runtime.embedding_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM\n",
      "---------------------------------\n",
      "Run id: 9R3MMU\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 9680\n",
      "Validation samples: 2421\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Operation 'LSTM/LSTM/cond/Assign' has been marked as not fetchable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-961866547896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         n_epoch=50)\n\u001b[0m",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    213\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    331\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mfeed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[1;32m    774\u001b[0m                                              feed_batch)\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tflearn/config.py\u001b[0m in \u001b[0;36mis_training\u001b[0;34m(is_training, session)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0minit_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_training_ops'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_training_ops'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3739\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3740\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3741\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/feltenberger/anaconda3/envs/pl35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_assert_fetchable\u001b[0;34m(self, graph, op)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m       raise ValueError(\n\u001b[0;32m--> 434\u001b[0;31m           'Operation %r has been marked as not fetchable.' % op.name)\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'LSTM/LSTM/cond/Assign' has been marked as not fetchable."
     ]
    }
   ],
   "source": [
    "print('Training LSTM')\n",
    "with tf.Graph().as_default():\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        trainY,\n",
    "        validation_set=(testX, testY),\n",
    "        show_metric=True,\n",
    "        batch_size=32,\n",
    "        n_epoch=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       seed       0.66      0.96      0.78       208\n",
      "   antiseed       1.00      0.95      0.97      2231\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2439\n",
      "\n",
      "[[ 199    9]\n",
      " [ 104 2127]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predictions_for_report(patent_model, validX, validY):\n",
    "    target_names = ['seed', 'antiseed']\n",
    "\n",
    "    prediction_scores = patent_model.predict(validX)\n",
    "    predictions = []\n",
    "    actual_y = []\n",
    "\n",
    "    for idx in range(0, len(prediction_scores)):\n",
    "        prediction = prediction_scores[idx]\n",
    "        actual = validY[idx]\n",
    "        predictions.append(prediction_to_idx(prediction))\n",
    "        actual_y.append(prediction_to_idx(actual))\n",
    "\n",
    "    return predictions, actual_y, target_names\n",
    "\n",
    "predictions, actual_y, target_names = predictions_for_report(model, testX, testY)\n",
    "cr = classification_report(predictions, actual_y, target_names=target_names)\n",
    "cm = confusion_matrix(predictions, actual_y)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_num</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>family_id</th>\n",
       "      <th>priority_date</th>\n",
       "      <th>title_lang</th>\n",
       "      <th>abstract_lang</th>\n",
       "      <th>claims_lang</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>title_text</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>description_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5546456</td>\n",
       "      <td>US-5546456-A</td>\n",
       "      <td>23091784</td>\n",
       "      <td>19940802</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Telecommunication system with inbound call res...</td>\n",
       "      <td>A telecommunication system (10) having an auto...</td>\n",
       "      <td>We claim: \\n       \\n       1. A telecommunica...</td>\n",
       "      <td>This application is a continuation of applicat...</td>\n",
       "      <td>Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5252489</td>\n",
       "      <td>US-5252489-A</td>\n",
       "      <td>25452087</td>\n",
       "      <td>19890117</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Down syndrome screening method utilizing dried...</td>\n",
       "      <td>The present invention relates to a method for ...</td>\n",
       "      <td>I claim: \\n       \\n       1. A screening meth...</td>\n",
       "      <td>This application is a continuation-in-part of ...</td>\n",
       "      <td>Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7818270</td>\n",
       "      <td>US-7818270-B2</td>\n",
       "      <td>36692900</td>\n",
       "      <td>20050118</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Method and system for tracking and budgeting e...</td>\n",
       "      <td>An energy tracking and reporting system can re...</td>\n",
       "      <td>1. A method of budgeting future energy consump...</td>\n",
       "      <td>RELATED APPLICATIONS \\n     This application c...</td>\n",
       "      <td>Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015112911</td>\n",
       "      <td>US-2015112911-A1</td>\n",
       "      <td>52827087</td>\n",
       "      <td>20131021</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Coupling parallel event-driven computation wit...</td>\n",
       "      <td>The present invention provides a system compri...</td>\n",
       "      <td>What is claimed is: \\n       \\n         1 . A ...</td>\n",
       "      <td>[0001]    This invention was made with Governm...</td>\n",
       "      <td>Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002087532</td>\n",
       "      <td>US-2002087532-A1</td>\n",
       "      <td>22981932</td>\n",
       "      <td>20001229</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Cooperative, interactive, heuristic system for...</td>\n",
       "      <td>An Internet-related invention comprising hardw...</td>\n",
       "      <td>What is claimed is:  \\n       \\n         1 . A...</td>\n",
       "      <td>RELATED CASE  \\n       [0001]    This Applicat...</td>\n",
       "      <td>Seed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_num publication_number family_id  priority_date title_lang  \\\n",
       "0     5546456       US-5546456-A  23091784       19940802         en   \n",
       "1     5252489       US-5252489-A  25452087       19890117         en   \n",
       "2     7818270      US-7818270-B2  36692900       20050118         en   \n",
       "3  2015112911   US-2015112911-A1  52827087       20131021         en   \n",
       "4  2002087532   US-2002087532-A1  22981932       20001229         en   \n",
       "\n",
       "  abstract_lang claims_lang description_lang  \\\n",
       "0            en          en               en   \n",
       "1            en          en               en   \n",
       "2            en          en               en   \n",
       "3            en          en               en   \n",
       "4            en          en               en   \n",
       "\n",
       "                                          title_text  \\\n",
       "0  Telecommunication system with inbound call res...   \n",
       "1  Down syndrome screening method utilizing dried...   \n",
       "2  Method and system for tracking and budgeting e...   \n",
       "3  Coupling parallel event-driven computation wit...   \n",
       "4  Cooperative, interactive, heuristic system for...   \n",
       "\n",
       "                                       abstract_text  \\\n",
       "0  A telecommunication system (10) having an auto...   \n",
       "1  The present invention relates to a method for ...   \n",
       "2  An energy tracking and reporting system can re...   \n",
       "3  The present invention provides a system compri...   \n",
       "4  An Internet-related invention comprising hardw...   \n",
       "\n",
       "                                         claims_text  \\\n",
       "0  We claim: \\n       \\n       1. A telecommunica...   \n",
       "1  I claim: \\n       \\n       1. A screening meth...   \n",
       "2  1. A method of budgeting future energy consump...   \n",
       "3  What is claimed is: \\n       \\n         1 . A ...   \n",
       "4  What is claimed is:  \\n       \\n         1 . A...   \n",
       "\n",
       "                                    description_text label  \n",
       "0  This application is a continuation of applicat...  Seed  \n",
       "1  This application is a continuation-in-part of ...  Seed  \n",
       "2  RELATED APPLICATIONS \\n     This application c...  Seed  \n",
       "3  [0001]    This invention was made with Governm...  Seed  \n",
       "4  RELATED CASE  \\n       [0001]    This Applicat...  Seed  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_df[training_df.abstract_text.str.contains('learn') & training_df.abstract_text.str.contains('machine')]\n",
    "\n",
    "#training_df[training_df.abstract_text.str.contains('machine')]\n",
    "l2_texts = expander.load_training_data_from_pubs(l2_patents_df[['publication_number']])\n",
    "l2_texts['label'] = 'Seed'\n",
    "l2_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       seed       0.20      1.00      0.33      2009\n",
      "   antiseed       1.00      0.00      0.00      7991\n",
      "\n",
      "avg / total       0.84      0.20      0.07     10000\n",
      "\n",
      "[[2009    0]\n",
      " [7990    1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "l1_texts_subset = l1_texts.abstract_text[:10000]\n",
    "l1_texts_subset.reset_index(drop=True, inplace=True)\n",
    "l1_texts_subset.loc[len(l1_texts_subset)-1] = 'this abstract is about something entirely boring'\n",
    "\n",
    "l1_labels_subset = l1_texts.label[:10000]\n",
    "l1_labels_subset.reset_index(drop=True, inplace=True)\n",
    "l1_labels_subset.loc[len(l1_texts_subset)-1] = 'AntiSeed'\n",
    "\n",
    "l1_x, l1_y = prep_series_for_training(w2v_runtime, l1_texts_subset, l1_labels_subset)\n",
    "l1_x = np.array(l1_x)\n",
    "l1_y = np.array(l1_y)\n",
    "\n",
    "\n",
    "# Convert text idx into padded sequences\n",
    "l1_x = pad_sequences(l1_x, maxlen=sequence_len, value=mask_idx)\n",
    "# Converting labels to binary vectors\n",
    "l1_y = to_categorical(l1_y, nb_classes=2)\n",
    "\n",
    "l1_preds, actual_l1_y, target_names = predictions_for_report(model, l1_x, l1_y)\n",
    "\n",
    "cr = classification_report(l1_preds, actual_l1_y, target_names=target_names)\n",
    "cm = confusion_matrix(l1_preds, actual_l1_y)\n",
    "print(cr)\n",
    "print(cm)\n",
    "#l1_texts_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>antiseed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PredictionIdx Prediction\n",
       "0                 1   antiseed\n",
       "1                 1   antiseed\n",
       "2                 1   antiseed\n",
       "3                 0       seed\n",
       "4                 1   antiseed\n",
       "5                 1   antiseed\n",
       "6                 1   antiseed\n",
       "7                 1   antiseed\n",
       "8                 1   antiseed\n",
       "9                 1   antiseed\n",
       "10                1   antiseed\n",
       "11                1   antiseed\n",
       "12                1   antiseed\n",
       "13                1   antiseed\n",
       "14                1   antiseed\n",
       "15                0       seed\n",
       "16                1   antiseed\n",
       "17                1   antiseed\n",
       "18                1   antiseed\n",
       "19                1   antiseed\n",
       "20                1   antiseed\n",
       "21                0       seed\n",
       "22                1   antiseed\n",
       "23                0       seed\n",
       "24                0       seed\n",
       "25                1   antiseed\n",
       "26                1   antiseed\n",
       "27                0       seed\n",
       "28                0       seed\n",
       "29                1   antiseed\n",
       "...             ...        ...\n",
       "9970              1   antiseed\n",
       "9971              1   antiseed\n",
       "9972              1   antiseed\n",
       "9973              1   antiseed\n",
       "9974              0       seed\n",
       "9975              1   antiseed\n",
       "9976              1   antiseed\n",
       "9977              1   antiseed\n",
       "9978              0       seed\n",
       "9979              1   antiseed\n",
       "9980              1   antiseed\n",
       "9981              1   antiseed\n",
       "9982              0       seed\n",
       "9983              1   antiseed\n",
       "9984              0       seed\n",
       "9985              1   antiseed\n",
       "9986              1   antiseed\n",
       "9987              1   antiseed\n",
       "9988              1   antiseed\n",
       "9989              1   antiseed\n",
       "9990              1   antiseed\n",
       "9991              1   antiseed\n",
       "9992              1   antiseed\n",
       "9993              1   antiseed\n",
       "9994              1   antiseed\n",
       "9995              0       seed\n",
       "9996              1   antiseed\n",
       "9997              1   antiseed\n",
       "9998              0       seed\n",
       "9999              1   antiseed\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(l1_preds)\n",
    "l1_preds_df = pd.DataFrame(l1_preds, columns=['PredictionIdx'])\n",
    "l1_preds_df['Prediction'] = l1_preds_df.PredictionIdx.apply(label_id_to_text)\n",
    "l1_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The present invention relates to a method for detecting fetal Down syndrome (Trisomy 21), trisomy 13, trisomy 18 and other chromosomal anomalies during prenatal screening by analyzing a dried blood sample from a pregnant woman. More particularly the present invention relates to a method for improving detection efficiency in screening for the anomalies by measuring the amount of the free beta human chorionic gonadotropin (HCG) and nicked or fragmented or aberrant forms of free beta (HCG), all of which are referenced throughout this application as free beta (HCG) in dried blood samples from pregnant women.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_texts_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd3d8c6f978>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGaCAYAAAAl57hmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd28aebbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm_df = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_prediction(patent_model, text, expected_label, max_seq_len, mask_idx):\n",
    "    inf_series = pd.Series([text])\n",
    "    lab_series = pd.Series([label])\n",
    "\n",
    "    prepped_train, prepped_labels = \\\n",
    "        prep_series_for_training(\n",
    "            w2v_runtime=w2v_runtime,\n",
    "            labels_series=lab_series,\n",
    "            raw_series_text=inf_series)\n",
    "\n",
    "    # make sure we have the correct sequence length\n",
    "    infX = pad_sequences(prepped_train, maxlen=max_seq_len, value=mask_idx)\n",
    "\n",
    "    # actually make prediction\n",
    "    prediction = patent_model.predict(infX)[0]\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Disclosed is a means and method for placing an implantable neurostimulator control module into a place in the cranium where cranial bone has been removed. The method for accomplishing this cranial implantation is by first removing a patient&#39;s hair over the site of the implant, then cutting the scalp at that site and pulling it back to expose the cranium. A neurosurgeon would then remove a portion of the cranial bone to accept a control module to be implanted within that hole. The control module would then be placed into that hole. It is also conceived that the control module would be fixed in place by the use of one or more attachment devices such as a multiplicity of bone screws placed through holes in one or more flanges that extend over the cranium beyond the control module. The implantation could also include a fairing placed around the control module to provide a smooth contour under the patient&#39;s scalp. Also described is a spacer shim placed under the flange(s) to adjust the position of the control module so that its bottom surface does not put pressure on the dura mater lying directly over the brain tissue at the bottom of the hole. It is also envisioned that a resorbable disk could be placed under the bottom surface of the control module to further protect the brain and/or to elute an anti-biotic or anti-inflammatory substance to reduce the possibility of infection and/or inflammation.\n",
      "Prediction: [0.005607507191598415, 0.9943925738334656]\n",
      "Predicted Label: antiseed, Actual Label: Seed\n"
     ]
    }
   ],
   "source": [
    "text = l1_texts_subset[10]#'a neurosynaptic processing device including multiple core circuits for parallel processing, and a serial processing device including at least one processor core for serial processing. Each core circuit comprises multiple electronic neurons interconnected with multiple electronic axons via a plurality of synapse devices. The system further comprises an interconnect circuit for coupling the processing device with the serial processing device. The interconnect circuit enables the exchange of data packets between the neurosynaptic processing device and the serial processing device'\n",
    "label = 'Seed'\n",
    "prediction = make_prediction(model, text, expected_label=label, max_seq_len=sequence_len, mask_idx=mask_idx)\n",
    "print('Text: {}'.format(text))\n",
    "#print('Tokenized/Integerized: {}'.format(prepped_train))\n",
    "#print('Padded: {}'.format(infX))\n",
    "print('Prediction: {}'.format(prediction))\n",
    "print('Predicted Label: {}, Actual Label: {}'.format(prediction_to_label(prediction), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#oldidx = print_idx\n",
    "#print_idx = oldidx\n",
    "#print(model.predict([trainX[print_idx]]))\n",
    "#tensor_label_to_text(trainY[print_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pl35]",
   "language": "python",
   "name": "conda-env-pl35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
